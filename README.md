# Awsome-Visual Question Answering

A list of resources for Visual Question Answering.

### Based On VQA 2.0(1.0) Dataset

![](https://visualqa.org/static/img/challenge.png)

##### ICCV2015

[1] Antol S, Agrawal A, Lu J, et al. Vqa: Visual question answering [[paper](http://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper.html)] [[project](https://visualqa.org/)] 



##### ICCV2017

[1] Ben-Younes H, Cadene R, Cord M, et al. Mutan: Multimodal tucker fusion for visual question answering [[paper](http://openaccess.thecvf.com/content_iccv_2017/html/Ben-younes_MUTAN_Multimodal_Tucker_ICCV_2017_paper.html)]

[2] Zhu C, Zhao Y, Huang S, et al. Structured attentions for visual question answering [[paper](http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Structured_Attentions_for_ICCV_2017_paper.html)] [[code](https://github.com/shtechair/vqa-sva)]

##### CVPR 2018

[1] Agrawal A, Batra D, Parikh D, et al. Don't just assume; look and answer: Overcoming priors for visual question answering [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Agrawal_Dont_Just_Assume_CVPR_2018_paper.pdf)]

[2] Anderson P, He X, Buehler C, et al. Bottom-up and top-down attention for image captioning and visual question answering [[paper](http://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.html)] [[code](https://github.com/peteanderson80/bottom-up-attention)]

##### ECCV2018

[1] Bai Y, Fu J, Zhao T, et al. Deep attention neural tensor network for visual question answering [[paper](http://openaccess.thecvf.com/content_ECCV_2018/html/Yalong_Bai_Deep_Attention_Neural_ECCV_2018_paper.html)]

##### CVPR 2019

[1] Cadene R, Ben-younes H, Cord M, et al. MUREL: Multimodal Relational Reasoning for Visual Question Answering [[paper](https://arxiv.org/abs/1902.09487)] [[code](https://github.com/Cadene/murel.bootstrap.pytorch)]

