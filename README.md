# Awsome-Visual Question Answering

A list of resources for Visual Question Answering.

![](https://visualqa.org/static/img/challenge.png)



##### ICCV 2015

[1] Antol S, Agrawal A, Lu J, et al. Vqa: Visual question answering [[paper](http://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper.html)] [[project](https://visualqa.org/)] 

##### NIPS2015

[1] Ren M, Kiros R, Zemel R. Exploring models and data for image question answering [[paper](http://papers.nips.cc/paper/5640-exploring-models-and-data-for-image-question-answering.pdf)] [[code](https://github.com/abhigoyal1997/CS-763-Project)]

##### CVPR2016

[1] Yang Z, He X, Gao J, et al. Stacked attention networks for image question answering [[paper](http://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf)] [[code](https://github.com/zcyang/imageqa-san)]

[2] Andreas J, Rohrbach M, Darrell T, et al. Neural module networks [[paper](http://openaccess.thecvf.com/content_cvpr_2016/papers/Andreas_Neural_Module_Networks_CVPR_2016_paper.pdf)] [[code](https://github.com/jacobandreas/nmn2)]

##### NIPS2016

[1] Lu J, Yang J, Batra D, et al. Hierarchical question-image co-attention for visual question answering [[paper](http://papers.nips.cc/paper/6202-hierarchical-question-image-co-attention-for-visual-question-answering.pdf)] [[code](https://github.com/jiasenlu/HieCoAttenVQA)]

##### EMNLP2016

[1] Fukui A, Park D H, Yang D, et al. Multimodal compact bilinear pooling for visual question answering and visual grounding [[paper](https://www.aclweb.org/anthology/D16-1044)] [[code](https://github.com/akirafukui/vqa-mcb)]

##### ECCV2016

[1] Jabri A, Joulin A, Van Der Maaten L. Revisiting visual question answering baselines [[paper](https://arxiv.org/pdf/1606.08390.pdf)]

##### CVIU(compute Visual and Image Understanding)2016

[1] Wu Q, Teney D, Wang P, et al. Visual question answering: A survey of methods and datasets [[paper](https://arxiv.org/pdf/1607.05910v1.pdf)] 

##### ArXiv 2016

[1] Kim J H, On K W, Lim W, et al. Hadamard product for low-rank bilinear pooling [[paper](https://arxiv.org/pdf/1610.04325.pdf)] [[code](https://github.com/Cadene/vqa.pytorch)]

##### CVPR 2017

[1] Goyal Y, Khot T, Summers-Stay D, et al. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering [[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Goyal_Making_the_v_CVPR_2017_paper.pdf)] 

[2] Johnson J, Hariharan B, van der Maaten L, et al. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning [[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Johnson_CLEVR_A_Diagnostic_CVPR_2017_paper.pdf)] 

[3] Ganju S, Russakovsky O, Gupta A. What's in a question: Using visual questions as a form of supervision [[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Ganju_Whats_in_a_CVPR_2017_paper.pdf)] [[code](https://github.com/sidgan/whats_in_a_question)] 

[4] Nam H, Ha J W, Kim J. Dual attention networks for multimodal reasoning and matching [[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Nam_Dual_Attention_Networks_CVPR_2017_paper.pdf)] [[code](https://github.com/JunweiLiang/DualAttentionNetwork)] 



##### LCPRIA2017(Iberian Conference on Pattern Recognition and Image Analysis)

[1] Bolaños M, Peris Á, Casacuberta F, et al. VIBIKNet: Visual bidirectional kernelized network for visual question answering [[paper](https://arxiv.org/pdf/1612.03628v1.pdf)] [[code](https://github.com/MarcBS/VIBIKNet)]

##### ICCV 2017

[1] Ben-Younes H, Cadene R, Cord M, et al. Mutan: Multimodal tucker fusion for visual question answering [[paper](http://openaccess.thecvf.com/content_iccv_2017/html/Ben-younes_MUTAN_Multimodal_Tucker_ICCV_2017_paper.html)] [[code](https://github.com/Cadene/vqa.pytorch)]

[2] Zhu C, Zhao Y, Huang S, et al. Structured attentions for visual question answering [[paper](http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Structured_Attentions_for_ICCV_2017_paper.html)] [[code](https://github.com/shtechair/vqa-sva)]

[3] Hu R, Andreas J, Rohrbach M, et al. Learning to reason: End-to-end module networks for visual question answering [[paper](http://openaccess.thecvf.com/content_ICCV_2017/papers/Hu_Learning_to_Reason_ICCV_2017_paper.pdf)]

[4] Yu Z, Yu J, Fan J, et al. Multi-modal factorized bilinear pooling with co-attention learning for visual question answering [[paper](http://openaccess.thecvf.com/content_ICCV_2017/papers/Yu_Multi-Modal_Factorized_Bilinear_ICCV_2017_paper.pdf)] [[code](https://github.com/yuzcccc/vqa-mfb)]

[5] Zhu C, Zhao Y, Huang S, et al. Structured attentions for visual question answering [[paper](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Structured_Attentions_for_ICCV_2017_paper.pdf)] [[code](https://github.com/shtechair/vqa-sva)]

[6] Gan C, Li Y, Li H, et al. Vqs: Linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation [[paper](http://openaccess.thecvf.com/content_ICCV_2017/papers/Gan_VQS_Linking_Segmentations_ICCV_2017_paper.pdf)] 

##### NIPS2017

[1] Schwartz I, Schwing A, Hazan T. High-order attention models for visual question answering [[paper](http://papers.nips.cc/paper/6957-high-order-attention-models-for-visual-question-answering.pdf)] [[code](https://github.com/idansc/HighOrderAtten)]

[2] Ilievski I, Feng J. Multimodal learning and reasoning for visual question answering [[paper](http://papers.nips.cc/paper/6658-multimodal-learning-and-reasoning-for-visual-question-answering.pdf)]



##### EMNLP 2017

Mahendru A, Prabhu V, Mohapatra A, et al. The promise of premise: Harnessing question premises in visual question answering [[paper](https://arxiv.org/pdf/1705.00601.pdf)] [[code](https://github.com/virajprabhu/premise-emnlp17)] 

##### CVPR 2018

[1] Agrawal A, Batra D, Parikh D, et al. Don't just assume; look and answer: Overcoming priors for visual question answering [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Agrawal_Dont_Just_Assume_CVPR_2018_paper.pdf)] [[code](https://github.com/AishwaryaAgrawal/GVQA)]

[2] Anderson P, He X, Buehler C, et al. Bottom-up and top-down attention for image captioning and visual question answering [[paper](http://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.html)] [[code](https://github.com/peteanderson80/bottom-up-attention)]

[3] Teney D, Anderson P, He X, et al. Tips and tricks for visual question answering: Learnings from the 2017 challenge [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Teney_Tips_and_Tricks_CVPR_2018_paper.pdf)]

[4] Gordon D, Kembhavi A, Rastegari M, et al. Iqa: Visual question answering in interactive environments [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Gordon_IQA_Visual_Question_CVPR_2018_paper.pdf)] [[code](https://github.com/danielgordon10/thor-iqa-cvpr-2018)]

[5] Nguyen D K, Okatani T. Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Nguyen_Improved_Fusion_of_CVPR_2018_paper.pdf)] [[code](https://github.com/cvlab-tohoku/Dense-CoAttention-Network)]

[6] Liang J, Jiang L, Cao L, et al. Focal visual-text attention for visual question answering [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Liang_Focal_Visual-Text_Attention_CVPR_2018_paper.pdf)] [[code](https://github.com/JunweiLiang/FVTA_MemexQA)]

[7] Huk Park D, Anne Hendricks L, Akata Z, et al. Multimodal explanations: Justifying decisions and pointing to the evidence [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Park_Multimodal_Explanations_Justifying_CVPR_2018_paper.pdf)]

[8] Gurari D, Li Q, Stangl A J, et al. Vizwiz grand challenge: Answering visual questions from blind people [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Gurari_VizWiz_Grand_Challenge_CVPR_2018_paper.pdf)]

[9] Mascharka D, Tran P, Soklaski R, et al. Transparency by design: Closing the gap between performance and interpretability in visual reasoning [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Mascharka_Transparency_by_Design_CVPR_2018_paper.pdf)]

[10] Cao Q, Liang X, Li B, et al. Visual question reasoning on general dependency tree [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Visual_Question_Reasoning_CVPR_2018_paper.pdf)]

[11] Patro B, Namboodiri V P. Differential attention for visual question answering [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Patro_Differential_Attention_for_CVPR_2018_paper.pdf)]

[12] Su Z, Zhu C, Dong Y, et al. Learning visual knowledge memory networks for visual question answering [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Su_Learning_Visual_Knowledge_CVPR_2018_paper.pdf)]

[13] Fan H, Zhou J. Stacked latent attention for multimodal reasoning [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Stacked_Latent_Attention_CVPR_2018_paper.pdf)]

[14] Hu H, Chao W L, Sha F. Learning answer embeddings for visual question answering [[paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Learning_Answer_Embeddings_CVPR_2018_paper.pdf)]



##### ICLR2018

[1] Zhang Y, Hare J, Prügel-Bennett A. Learning to count objects in natural images for visual question answering [[paper](https://arxiv.org/pdf/1802.05766.pdf)] [[code](https://github.com/Cyanogenoid/vqa-counting)]

##### AAAI 2018

[1] Lu P, Li H, Zhang W, et al. Co-attending free-form regions and detections with multi-modal multiplicative feature embedding for visual question answering [[paper](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16249/16315)] [[code](https://github.com/lupantech/dual-mfa-vqa)]

##### ACL 2018

[1] Mudrakarta P K, Taly A, Sundararajan M, et al. Did the model understand the question? [[paper](https://arxiv.org/pdf/1805.05492v1.pdf)]

##### Transactions on neural networks and learning systems2018

[1] Yu Z, Yu J, Xiang C, et al. Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering [[paper](https://arxiv.org/pdf/1708.03619.pdf)] [[code](https://github.com/yuzcccc/vqa-mfb)] 

##### ECCV 2018

[1] Bai Y, Fu J, Zhao T, et al. Deep attention neural tensor network for visual question answering [[paper](http://openaccess.thecvf.com/content_ECCV_2018/html/Yalong_Bai_Deep_Attention_Neural_ECCV_2018_paper.html)]

[2] Yang G R, Ganichev I, Wang X J, et al. A dataset and architecture for visual reasoning with a working memory [[paper](http://openaccess.thecvf.com/content_ECCV_2018/papers/Guangyu_Robert_Yang_A_dataset_and_ECCV_2018_paper.pdf)] 

[3] Li Q, Tao Q, Joty S, et al. Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions [[paper](http://openaccess.thecvf.com/content_ECCV_2018/papers/Qing_Li_VQA-E_Explaining_Elaborating_ECCV_2018_paper.pdf)]

[4] Shi Y, Furlanello T, Zha S, et al. Question type guided attention in visual question answering [[paper](http://openaccess.thecvf.com/content_ECCV_2018/papers/Yang_Shi_Question_Type_Guided_ECCV_2018_paper.pdf)]

[5] Malinowski M, Doersch C, Santoro A, et al. Learning visual question answering by bootstrapping hard attention [[paper](http://openaccess.thecvf.com/content_ECCV_2018/papers/Mateusz_Malinowski_Learning_Visual_Question_ECCV_2018_paper.pdf)]

[6] Yu Y, Kim J, Kim G. A joint sequence fusion model for video question answering and retrieval [[paper](http://openaccess.thecvf.com/content_ECCV_2018/papers/Youngjae_Yu_A_Joint_Sequence_ECCV_2018_paper.pdf)]

[7] Gao P, Li H, Li S, et al. Question-guided hybrid convolution for visual question answering [[paper](http://openaccess.thecvf.com/content_ECCV_2018/papers/gao_peng_Question-Guided_Hybrid_Convolution_ECCV_2018_paper.pdf)]

[8] Narasimhan M, Schwing A G. Straight to the facts: Learning knowledge base retrieval for factual visual question answering [[paper](http://openaccess.thecvf.com/content_ECCV_2018/papers/Medhini_Gulganjalli_Narasimhan_Straight_to_the_ECCV_2018_paper.pdf)]

[9] Li W, Yuan Z, Fang X, et al. Knowing Where to Look? Analysis on Attention of Visual Question Answering System [[paper](http://openaccess.thecvf.com/content_ECCVW_2018/papers/11132/Li_Knowing_Where_to_Look_Analysis_on_Attention_of_Visual_Question_ECCVW_2018_paper.pdf)]

##### NIPS2018

[1] Kim J H, Jun J, Zhang B T. Bilinear attention networks [[paper](http://papers.nips.cc/paper/7429-bilinear-attention-networks.pdf)] [[code](https://github.com/jnhwkim/ban-vqa)]

[2] Norcliffe-Brown W, Vafeias S, Parisot S. Learning conditioned graph structures for interpretable visual question answering [[paper](http://papers.nips.cc/paper/8054-learning-conditioned-graph-structures-for-interpretable-visual-question-answering.pdf)] [[code](https://github.com/aimbrain/vqa-project)]

[3] Deng Y, Kim Y, Chiu J, et al. Latent alignment and variational attention [[paper](http://papers.nips.cc/paper/8179-latent-alignment-and-variational-attention.pdf)] [[code](https://github.com/harvardnlp/var-attn)] 

[4] Yi K, Wu J, Gan C, et al. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding [[paper](http://papers.nips.cc/paper/7381-neural-symbolic-vqa-disentangling-reasoning-from-vision-and-language-understanding.pdf)] [[code](https://github.com/kexinyi/ns-vqa)]

[5] Narasimhan M, Lazebnik S, Schwing A. Out of the box: Reasoning with graph convolution nets for factual visual question answering [[paper](http://papers.nips.cc/paper/7531-out-of-the-box-reasoning-with-graph-convolution-nets-for-factual-visual-question-answering.pdf)]

[6] Wu C, Liu J, Wang X, et al. Chain of reasoning for visual question answering [[paper](http://papers.nips.cc/paper/7311-chain-of-reasoning-for-visual-question-answering.pdf)]

##### ArXiv 2018

[1] Jiang Y, Natarajan V, Chen X, et al. Pythia v0. 1: the winning entry to the vqa challenge 2018 [[paper](https://arxiv.org/pdf/1807.09956.pdf)] [[code](https://arxiv.org/pdf/1807.09956.pdf)] 

##### CVPR 2019

[1] Cadene R, Ben-younes H, Cord M, et al. MUREL: Multimodal Relational Reasoning for Visual Question Answering [[paper](https://arxiv.org/abs/1902.09487)] [[code](https://github.com/Cadene/murel.bootstrap.pytorch)]

[2] Peng G, Li H, You H, et al. Dynamic Fusion with Intra-and Inter-Modality Attention Flow for Visual Question Answering [[paper](https://arxiv.org/abs/1812.05252)] [[code](https://github.com/KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch)]

[3] Shah M, Chen X, Rohrbach M, et al. Cycle-Consistency for Robust Visual Question Answering [[paper](https://arxiv.org/pdf/1902.05660.pdf)]

[4] Marino K, Rastegari M, Farhadi A, et al. OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge [[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Marino_OK-VQA_A_Visual_Question_Answering_Benchmark_Requiring_External_Knowledge_CVPR_2019_paper.pdf)]

[5] Li H, Wang P, Shen C, et al. Visual Question Answering as Reading Comprehension[[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Visual_Question_Answering_as_Reading_Comprehension_CVPR_2019_paper.pdf)]

[6] Kim J, Ma M, Kim K, et al. Progressive Attention Memory Network for Movie Story Question Answering[[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Progressive_Attention_Memory_Network_for_Movie_Story_Question_Answering_CVPR_2019_paper.pdf)]

[7] Manjunatha V, Saini N, Davis L S. Explicit Bias Discovery in Visual Question Answering Models[[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Manjunatha_Explicit_Bias_Discovery_in_Visual_Question_Answering_Models_CVPR_2019_paper.pdf)]

[8] Shrestha R, Kafle K, Kanan C. Answer them all! toward universal visual question answering models[[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Shrestha_Answer_Them_All_Toward_Universal_Visual_Question_Answering_Models_CVPR_2019_paper.pdf)]

[9] Singh A, Natarajan V, Shah M, et al. Towards vqa models that can read[[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Singh_Towards_VQA_Models_That_Can_Read_CVPR_2019_paper.pdf)]

[10] Fan C, Zhang X, Zhang S, et al. Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering[[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Fan_Heterogeneous_Memory_Enhanced_Multimodal_Attention_Model_for_Video_Question_Answering_CVPR_2019_paper.pdf)]

[11] Fukui H, Hirakawa T, Yamashita T, et al. Attention branch network: Learning of attention mechanism for visual explanation [[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Fukui_Attention_Branch_Network_Learning_of_Attention_Mechanism_for_Visual_Explanation_CVPR_2019_paper.pdf)]

[12] Xiong P, Zhan H, Wang X, et al. Visual Query Answering by Entity-Attribute Graph Matching and Reasoning [[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Xiong_Visual_Query_Answering_by_Entity-Attribute_Graph_Matching_and_Reasoning_CVPR_2019_paper.pdf)]

[13] Noh H, Kim T, Mun J, et al. Transfer Learning via Unsupervised Task Discovery for Visual Question Answering [[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Noh_Transfer_Learning_via_Unsupervised_Task_Discovery_for_Visual_Question_Answering_CVPR_2019_paper.pdf)] [[code](https://github.com/HyeonwooNoh/vqa_task_discovery)]

[14] Tang K, Zhang H, Wu B, et al. Learning to compose dynamic tree structures for visual contexts [[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Tang_Learning_to_Compose_Dynamic_Tree_Structures_for_Visual_Contexts_CVPR_2019_paper.pdf)]

[15] Yu Z, Yu J, Cui Y, et al. Deep Modular Co-Attention Networks for Visual Question Answering [[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Deep_Modular_Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.pdf)] [[code](https://github.com/MILVLG/mcan-vqa)]

##### ICLR2019

[1] Zhang Y, Hare J, Prügel-Bennett A. Learning Representations of Sets through Optimized Permutations [[paper](https://arxiv.org/pdf/1812.03928.pdf)] [[code](https://github.com/Cyanogenoid/perm-optim)]

##### TPAMI 2019

[1] Liang J, Jiang L, Cao L, et al. Focal visual-text attention for memex question answering [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8603827)] [[code](https://github.com/JunweiLiang/FVTA_MemexQA)]

##### AAAI 2019

[1] Ben-Younes H, Cadene R, Thome N, et al. BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection [[paper](https://arxiv.org/pdf/1902.00038.pdf)] [[code](https://arxiv.org/pdf/1902.00038.pdf)]

##### ArXiv2019

[1] Wu J, Mooney R J. Self-Critical Reasoning for Robust Visual Question Answering [[paper](https://arxiv.org/pdf/1905.09998v1.pdf)] [[code](https://github.com/jialinwu17/Self_Critical_VQA)]

[2] Cadene R, Dancette C, Ben-younes H, et al. RUBi: Reducing Unimodal Biases in Visual Question Answering [[paper](https://arxiv.org/pdf/1906.10169.pdf)] [[code](https://github.com/cdancette/rubi.bootstrap.pytorch)]

[3] Li L, Gan Z, Cheng Y, et al. Relation-aware Graph Attention Network for Visual Question Answering [[paper](https://arxiv.org/pdf/1903.12314.pdf)]

[4] Wu Y, Sun Q, Ma J, et al. Question Guided Modular Routing Networks for Visual Question Answering [[paper](https://arxiv.org/pdf/1904.08324.pdf)]



### Based On [GQA](https://cs.stanford.edu/people/dorarad/gqa/index.html) dataset

#### CVPR2019

[1] Hudson D A, Manning C D. Gqa: A new dataset for real-world visual reasoning and compositional question answering [[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Hudson_GQA_A_New_Dataset_for_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.pdf)] [[code](https://github.com/stanfordnlp/mac-network)]

